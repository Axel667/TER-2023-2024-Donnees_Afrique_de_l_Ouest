{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b05d46-629c-4728-b689-633ac73e0213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from transformers import FlaubertModel, FlaubertTokenizer\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84967134-93ad-471b-bac5-7d6e98cdad37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at flaubert/flaubert_base_cased were not used when initializing FlaubertModel: ['pred_layer.proj.weight', 'pred_layer.proj.bias']\n",
      "- This IS expected if you are initializing FlaubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing FlaubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Chargement de FlauBERT pour la tokenisation et les embeddings\n",
    "tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')\n",
    "model = FlaubertModel.from_pretrained('flaubert/flaubert_base_cased')\n",
    "\n",
    "def extract_articles_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Pattern pour identifier les codes d'article\n",
    "    mixed_code_pattern = re.compile(r'\\b(?=[A-Z0-9]{10}\\b)(?=[A-Z]*[0-9][A-Z]*)(?=[0-9]*[A-Z][0-9]*)[A-Z0-9]{10}')\n",
    "\n",
    "    # Extraction et comptage des codes uniques\n",
    "    unique_mixed_found_codes = set(mixed_code_pattern.findall(content))\n",
    "\n",
    "    # Détermination des limites des articles\n",
    "    article_boundaries = list(mixed_code_pattern.finditer(content))\n",
    "\n",
    "    # Extraction des articles\n",
    "    articles = []\n",
    "    for i in range(len(article_boundaries)):\n",
    "        start_index = article_boundaries[i].start()\n",
    "        end_index = article_boundaries[i + 1].start() if i + 1 < len(article_boundaries) else None\n",
    "        article_code = content[start_index:article_boundaries[i].end()].strip()\n",
    "        article_text = content[article_boundaries[i].end():end_index].strip()\n",
    "        articles.append((article_code, article_text))\n",
    "\n",
    "    return pd.DataFrame(articles, columns=['Code', 'Text'])\n",
    "\n",
    "def get_embeddings(text):\n",
    "    # Diviser le texte en segments de 512 tokens\n",
    "    max_length = 512\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    token_segments = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]\n",
    "    \n",
    "    embeddings_list = []\n",
    "    for segment in token_segments:\n",
    "        segment_tensor = torch.tensor([segment])\n",
    "        with torch.no_grad():\n",
    "            segment_embeddings = model(segment_tensor)[0]\n",
    "        embeddings_list.append(segment_embeddings.mean(dim=1).squeeze())\n",
    "    \n",
    "    # Prendre la moyenne des embeddings de tous les segments\n",
    "    return torch.mean(torch.stack(embeddings_list), dim=0)\n",
    "\n",
    "\n",
    "def vectorize_articles_with_embeddings(articles_df):\n",
    "    embeddings_list = []\n",
    "    for _, row in articles_df.iterrows():\n",
    "        embeddings = get_embeddings(row['Text'])\n",
    "        embeddings_list.append(embeddings.numpy())\n",
    "    \n",
    "    return pd.DataFrame(embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922af675-757f-4376-837d-8a3adb2db128",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\33672\\OneDrive\\Bureau\\TER Docs\\corpus_FRESA - Texte - Pertinence\\Entrainement - txt\\Non Pertinent.txt\"\n",
    "articles_df = extract_articles_from_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4a5f1d-44b5-4250-98b8-4183c3ff700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##UNIQUEMENT POUR DATASET ENTRAINEMENT\n",
    "\n",
    "# Ajouter une nouvelle colonne contenant des '1 ou 0' au début du DataFrame pour déterminer la pertinence ou non de l'article\n",
    "articles_df.insert(0, 'Pertinence', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ca93604-5889-4e9e-9bb1-0bb1c5599365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pertinence</th>\n",
       "      <th>Code</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>IWUSP7B1UT</td>\n",
       "      <td>,Bénin : Le gouvernement recrute 100 auditeurs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>HFJB1RBOBG</td>\n",
       "      <td>,Tirs de missiles : La Corée du Nord s'amélior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1HI8MCH2VW</td>\n",
       "      <td>,\"Poutine : Biden \"\"n'a aucune intention de s'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>J6YEAEEZXN</td>\n",
       "      <td>,Coupe du monde féminine 2023 : Les représenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3IT7YAZNRY</td>\n",
       "      <td>,Bénin: la réhabilitation des voies démarre fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pertinence        Code                                               Text\n",
       "0           0  IWUSP7B1UT  ,Bénin : Le gouvernement recrute 100 auditeurs...\n",
       "1           0  HFJB1RBOBG  ,Tirs de missiles : La Corée du Nord s'amélior...\n",
       "2           0  1HI8MCH2VW  ,\"Poutine : Biden \"\"n'a aucune intention de s'...\n",
       "3           0  J6YEAEEZXN  ,Coupe du monde féminine 2023 : Les représenta...\n",
       "4           0  3IT7YAZNRY  ,Bénin: la réhabilitation des voies démarre fi..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81897cb-e8fb-42c5-a240-d83d7abacce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1664 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Vectorisation des articles avec FlauBERT\n",
    "embeddings_df = vectorize_articles_with_embeddings(articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0aa2892-e3df-445f-b650-77398db974eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténer les embeddings avec la colonne de pertinence\n",
    "final_df = pd.concat([articles_df['Pertinence'], embeddings_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13436dd1-24d8-40c6-9d92-83fda9a53720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame sauvegardé au chemin : C:\\Users\\33672\\OneDrive\\Bureau\\TER Docs\\corpus_FRESA - Texte - Pertinence\\Entrainement - txt\\Non Pertinent_traité.pkl\n"
     ]
    }
   ],
   "source": [
    "# Extraire le nom de base du fichier\n",
    "base_name = os.path.basename(file_path)\n",
    "\n",
    "# Retirer l'extension du fichier et ajouter '_traité'\n",
    "new_file_name = os.path.splitext(base_name)[0] + '_traité.pkl'\n",
    "\n",
    "# Définir le chemin du nouveau fichier pickle\n",
    "new_file_path = os.path.join(os.path.dirname(file_path), new_file_name)\n",
    "\n",
    "# Sauvegarder le DataFrame au format pickle\n",
    "final_df.to_pickle(new_file_path)\n",
    "\n",
    "print(f\"DataFrame sauvegardé au chemin : {new_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e70f4b4-35ec-44dc-85ad-3be2d38ab9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Pertinence         0         1         2         3         4         5  \\\n",
      "0           1 -0.129774  0.448794  0.069120  0.804145 -0.059709 -0.303203   \n",
      "1           1 -0.628227  0.290821  0.605764  0.407965 -0.393719 -0.599755   \n",
      "2           1 -0.496422  0.360855  0.579142 -0.378369 -0.553538 -0.472761   \n",
      "3           1 -0.546441  0.814615  0.384502  0.502827 -0.206296 -0.550668   \n",
      "4           1 -0.333643  0.460296  0.445026 -0.007603 -0.297541  0.055417   \n",
      "\n",
      "          6         7         8  ...       758       759       760       761  \\\n",
      "0  0.070819 -0.688821 -0.257140  ... -0.298196  0.017583 -0.561512 -0.028051   \n",
      "1  0.487397 -0.738708 -0.271538  ... -0.419414  0.100172 -0.552149 -0.145568   \n",
      "2 -0.035669 -0.806150 -0.144847  ...  0.009380  0.239444 -0.026951 -0.035498   \n",
      "3 -0.083413 -0.470302 -0.522546  ... -0.491874 -0.167929 -0.415632 -0.331937   \n",
      "4  0.070353 -0.910181  0.218543  ... -0.201357 -0.300444 -0.477405 -0.381431   \n",
      "\n",
      "        762       763       764       765       766       767  \n",
      "0 -0.436011  0.552108 -0.861965 -1.712046 -0.125331  0.210531  \n",
      "1 -0.314757  0.074044 -0.613059 -1.340611  0.258310  0.195969  \n",
      "2 -0.426874 -0.328159 -0.233567 -1.055442  0.132318  0.332207  \n",
      "3 -0.622091  0.447335 -0.931246 -0.591714 -0.180659 -0.274223  \n",
      "4 -0.840143 -0.096706 -0.503710 -0.445824 -0.338096  0.438174  \n",
      "\n",
      "[5 rows x 769 columns]\n",
      "DataFrame sauvegardé avec succès à l'emplacement : C:/Users/33672/OneDrive/Bureau/TER Docs/corpus_FRESA - Texte - Pertinence/Entrainement - txt/train-traité.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##UNIQUEMENT POUR DATASET ENTRAINEMENT\n",
    "\n",
    "# Chemins vers les fichiers pickle\n",
    "pertinent_file_path = r\"C:\\Users\\33672\\OneDrive\\Bureau\\TER Docs\\corpus_FRESA - Texte - Pertinence\\Entrainement - txt\\Pertinent_traité.pkl\"\n",
    "non_pertinent_file_path = r\"C:\\Users\\33672\\OneDrive\\Bureau\\TER Docs\\corpus_FRESA - Texte - Pertinence\\Entrainement - txt\\Non Pertinent_traité.pkl\"\n",
    "\n",
    "# Charger les DataFrames à partir des fichiers pickle\n",
    "pertinent_df = pd.read_pickle(pertinent_file_path)\n",
    "non_pertinent_df = pd.read_pickle(non_pertinent_file_path)\n",
    "\n",
    "# Concaténer les deux DataFrames\n",
    "combined_df = pd.concat([pertinent_df, non_pertinent_df], ignore_index=True)\n",
    "\n",
    "# (Optionnel) Réinitialiser l'index du DataFrame combiné\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Affichage des premières lignes pour vérification\n",
    "print(combined_df.head())\n",
    "\n",
    "# Chemin pour enregistrer le DataFrame combiné\n",
    "save_path = r\"C:/Users/33672/OneDrive/Bureau/TER Docs/corpus_FRESA - Texte - Pertinence/Entrainement - txt/train-traité.pkl\"\n",
    "\n",
    "# Sauvegarde du DataFrame combiné\n",
    "combined_df.to_pickle(save_path)\n",
    "\n",
    "print(\"DataFrame sauvegardé avec succès à l'emplacement : \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ef0b60-0950-4b61-8fbd-99f0ab41d140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
