\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage{tabularx}
\usepackage{geometry}
\usepackage{booktabs}

% Set page margins
\geometry{left=25mm,right=25mm,top=20mm,bottom=20mm}

\title{Réflexion et Planification du Pipeline sur la partie entre le prétraitement textuel et la géolocalisation des articles:}
\author{Ivan Can Arisoy}
\date{\today}

\begin{document}

\maketitle

\section{Prétraitement des Données Textuelles}

\subsection{Document Embeddings}
\begin{itemize}
  \item Tokenisation : Utilisation du tokenizer CamemBERT pour segmenter chaque document du corpus.
  \item Embeddings : Passage de chaque document tokenisé à travers CamemBERT pour obtenir les embeddings.
  \item Moyennisation des embeddings : Moyennisation des embeddings de chaque token dans un document pour obtenir un vecteur d'embedding unique pour le document entier.
  \item Lemmatisation : Conversion de chaque mot à sa forme de base.
  \item Suppression des mots vides : Élimination des mots non pertinents.
  \item Vectorisation (TF-IDF) : Transformation du texte en vecteurs numériques avec TF-IDF.
\end{itemize}

\section{Filtrage de Pertinence}

Utilisation du modèle FlauBERT pour évaluer la pertinence du texte par rapport au lexique de la sécurité alimentaire et conservation des documents pertinents pour la question à l'étude.

\section{Analyse de Sentiment*}

Avant le clustering, utilisation d'un modèle pré-entraîné tel que BERT pour réaliser une analyse de sentiment sur les documents.

\section{Modélisation des Sujets}

\subsection{Clustering}
\begin{enumerate}
  \item Conversion des embeddings documentaires en une matrice pour le clustering.
  \item Utilisation d'un algorithme de clustering (par exemple, KMeans) pour regrouper les embeddings.
  \item Attribution de chaque document à son cluster respectif, représentant un sujet ou thème particulier.
\end{enumerate}


\section*{Enrichissement du Lexique}
Il existe peu de vocabulaire du lexique de la sécurité alimentaire en Français disponible sur Internet. Une solution consisterait à enrichir le lexique manuellement en ajoutant des mots au dictionnaire. On pourrait également envisager le « web scraping » pour compléter notre lexique.


\section*{Planification des Tâches}

\noindent
\begin{tabularx}{\textwidth}{@{}lXr@{}}
\toprule
\textbf{Étape} & \textbf{Description} & \textbf{Dates} \\
\midrule
1. Prétraitement des données textuelles & 
Enrichissement du lexique: 1 jour\newline
Tokenization: 1 jour\newline
Embeddings et Moyennisation des embeddings: 1 jour\newline
Lemmatisation, Suppression des mots vides, et Vectorisation (TF-IDF): 1 jour & 
03/11 - 06/11 \\
\midrule
2. Filtrage de pertinence & 
Configuration de FlauBERT: 0,5 jour\newline
Évaluation et filtrage des documents avec le lexique: 1,5 jour & 
07/11 - 08/11 \\
\midrule
3. Analyse de sentiment & 
Configuration de BERT pour l'analyse de sentiment: 0,5 jour\newline
Analyse de sentiment des documents: 0,5 jour & 
09/11 \\
\midrule
4. Modélisation des sujets & 
Clustering des embeddings documentaires: 2 jour\newline
Attribution des documents aux clusters: 1 jour & 
10/11 - 13/11 \\
\midrule
5. Analyse et Interprétation des Clusters & 
Etudes de chaque cluster: 2 jour\newline
Visualisation des clusters: 1 jour & 
13/11 - 15/11 \\
\midrule
6. Géolocalisation et visualisation des Articles & 
 & 
15/11 - 17/11 \\
\bottomrule
\end{tabularx}

\end{document}