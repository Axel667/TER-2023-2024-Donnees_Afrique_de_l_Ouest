{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "\n",
    "# Hugging Face Transformers \n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import TrainerCallback\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "# Scikit-learn packages for modeling and evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#!pip install GPUtil\n",
    "'''\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "from numba import cuda\n",
    "import string\n",
    "import re\n",
    "\n",
    "def free_gpu_cache():\n",
    "    print(\"Initial GPU Usage\")\n",
    "    gpu_usage()                             \n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    cuda.select_device(0)\n",
    "\n",
    "    print(\"GPU Usage after emptying the cache\")\n",
    "    gpu_usage()\n",
    "\n",
    "free_gpu_cache()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for Kaggle input directory\n",
    "base_path = '/kaggle/input/projet-ter/'  # Update this path if it's different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = [\n",
    "    \"articles_24 Heures au BCnin.www.24haubenin.info_.csv\",\n",
    "    \"articles_Africa 24.www.youtube.com_channel_UCmEcEP_oCZJ6Mr1uxhUFyRg.csv\",\n",
    "    \"articles_Agence de Presse Sngalaise Youtube.www.youtube.com_channel_UC8uoOv4RSzdZKlmPTguYjtg.csv\",\n",
    "    \"articles_Agence de Presse Sngalaise.www.aps.sn_.csv\",\n",
    "    \"articles_Banouto Media.www.youtube.com_channel_UCwsXkG4LatsF7u_b8dU1p-w.csv\",\n",
    "    \"articles_Banouto.www.banouto.bj_.csv\",\n",
    "    \"articles_Burkina24 Youtube.www.youtube.com_channel_UCJtaDORHQO20XA-tFwpJysQ.csv\",\n",
    "    \"articles_Burkina24.burkina24.com_.csv\",\n",
    "    \"articles_Dakaractu TV HD.www.youtube.com_channel_UCG0t6XiAHui-ziz7SwFTN0g.csv\",\n",
    "    \"articles_Dakaractu.www.dakaractu.com_.csv\",\n",
    "    \"articles_Fraternit.www.fraternitebj.info_.csv\",\n",
    "    \"articles_JeuneAfrique Youtube.www.youtube.com_channel_UCWkbzzrku8lwKK6DoBl4yTg.csv\",\n",
    "    \"articles_JeuneAfrique.www.jeuneafrique.com_.csv\",\n",
    "    \"articles_LObs.www.lobs.sn_.csv\",\n",
    "    \"articles_La Nation.lanation.bj_.csv\",\n",
    "    \"articles_La Nouvelle Tribune.lanouvelletribune.info_.csv\",\n",
    "    \"articles_Le Matinal.groupelematinal.com_category_actualites_.csv\",\n",
    "    \"articles_Le Quotidien.lequotidien.sn_.csv\",\n",
    "    \"articles_ORTB.www.youtube.com_channel_UCmPXzeJaO7nrA87GIz3N6wQ.csv\",\n",
    "    \"articles_RTB - Radiodiffusion Tlvision du Burkina.www.youtube.com_channel_UCZl9utbYlPMssMhgrGUqXZA.csv\",\n",
    "    \"articles_SIKKA TELEVISION.www.youtube.com_channel_UCplwKOWLV8s2XZBMsimOjvg.csv\",\n",
    "    \"articles_Senegal7.www.youtube.com_channel_UC5eVGjO4ITJA1KM_tva_OSQ.csv\",\n",
    "    \"articles_Sud Quotidien TV.www.youtube.com_channel_UCD-YykHgK3BOvnqFlcOFXIA.csv\",\n",
    "    \"articles_Sud Quotidien.www.sudquotidien.sn_.csv\",\n",
    "    \"articles_TFM (Tl Futurs Medias).www.youtube.com_channel_UC5NQ49FVRIAuWE1el6L2gkg.csv\",\n",
    "    \"articles_aCotonou Youtube.www.youtube.com_channel_UCdNz-U4WJwFvbMDMdgYNMrg.csv\",\n",
    "    \"articles_aCotonou.news.acotonou.com_.csv\",\n",
    "    \"articles_le faso.lefaso.net_.csv\",\n",
    "    \"articles_le soleil.lesoleil.sn_.csv\",\n",
    "]\n",
    "\n",
    "# Initialiser une liste vide pour stocker des trames de données\n",
    "dfs = []\n",
    "\n",
    "# Boucle principale\n",
    "for file_name in file_names:\n",
    "    file_path = f\"{base_path}{file_name}\"\n",
    "    \n",
    "    #  Lire chaque fichier dans une trame de données\n",
    "    if file_name.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'source' in df.columns and not df['source'].str.contains('youtube.com').any():\n",
    "            dfs.append(df)\n",
    "\n",
    "# Concaténer toutes les données en une seule\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\nTotal number of records in the combined dataframe: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest = ['CLS_Lexique Sécurité Alimentaire', 'CLS_Relevance', 'CLS_Usefulness', 'CLS_Relevance Yes/No']\n",
    "\n",
    "# Printer des valeurs uniques pour chaque colonne\n",
    "for col in cols_of_interest:\n",
    "    unique_values = df[col].unique()\n",
    "    print(f\"Unique values in '{col}': {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest = ['CLS_Lexique Sécurité Alimentaire', 'CLS_Relevance', 'CLS_Usefulness', 'CLS_Relevance Yes/No']\n",
    "\n",
    "# Counting and printing unique values for each column of interest\n",
    "for col in cols_of_interest:\n",
    "    print(f\"Counts of unique values in '{col}':\")\n",
    "    print(df[col].value_counts(dropna=False))  # Including NaN values in the count\n",
    "    print()  # Just to add an empty line for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['CLS_Relevance Yes/No'].isin(['No (u)', 'Yes (u)'])]\n",
    "\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivan_df = pd.read_csv('/kaggle/input/projet-ter/pertinence_Ivan.csv', sep = ';')\n",
    "# Drop rows where 'label' column has NaN values\n",
    "ivan_df = ivan_df.dropna(subset=['label'])\n",
    "\n",
    "ivan_df = ivan_df[['id', 'label', 'text']]\n",
    "ivan_df['label'] = ivan_df['label'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "ivan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guilhem_df = pd.read_csv('/kaggle/input/projet-ter/PERTINENCE MAIN GUILHEM.csv', sep = ';')\n",
    "guilhem_df = guilhem_df[['PERTINENCE MAIN GUILHEM', 'id', 'text']]\n",
    "guilhem_df = guilhem_df.rename(columns={'PERTINENCE MAIN GUILHEM': 'label'})\n",
    "\n",
    "guilhem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[['CLS_Relevance Yes/No', 'id', 'text']].rename(columns={'CLS_Relevance Yes/No': 'label'})\n",
    "\n",
    "filtered_df['label'] = filtered_df['label'].map({'No (u)': 0, 'Yes (u)': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = pd.concat([guilhem_df, ivan_df, filtered_df], ignore_index=True)\n",
    "# Finding duplicate ids with different labels\n",
    "duplicates = labeled_df.groupby('id').filter(lambda x: len(x['label'].unique()) > 1)\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(\"There are duplicate ids with different labels:\")\n",
    "    print(duplicates)\n",
    "else:\n",
    "    print(\"No duplicate ids with conflicting labels found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df = labeled_df.drop_duplicates(subset='id', keep='first')\n",
    "\n",
    "# Verify the result by checking for duplicates again, this should now come up empty for conflicting labels\n",
    "duplicates_check = labeled_df.groupby('id').filter(lambda x: len(x['label'].unique()) > 1)\n",
    "\n",
    "if not duplicates_check.empty:\n",
    "    print(\"There are still duplicate ids with different labels:\")\n",
    "    print(duplicates_check[['id', 'label']])\n",
    "else:\n",
    "    print(\"No duplicate ids with conflicting labels found after cleanup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifiez la distribution dans l'ensemble d'entrainement \n",
    "labeled_df = labeled_df.dropna(subset=['label'])\n",
    "labeled_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'label': ['0', '1', 'Pertinence main', 0, 1]\n",
    "}\n",
    "\n",
    "labeled_df = labeled_df[labeled_df['label'] != 'Pertinence main']\n",
    "\n",
    "# Convert 'label' to integer\n",
    "labeled_df['label'] = labeled_df['label'].astype(int)\n",
    "\n",
    "labeled_df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'label' column to integers\n",
    "labeled_df['label'] = labeled_df['label'].astype(int)\n",
    "\n",
    "labeled_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = df[['CLS_Relevance Yes/No', 'id', 'text']].rename(columns={'CLS_Relevance Yes/No': 'label'})\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['label'] = None\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df[~combined_df[['id', 'text']].apply(tuple, 1).isin(labeled_df[['id', 'text']].apply(tuple, 1))]\n",
    "\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device('cuda')\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camambert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "######labeled_df\n",
    "\n",
    "# Split the initial DataFrame into train+validation and test sets\n",
    "train_val_df, test_df = train_test_split(labeled_df, test_size=0.2, stratify=labeled_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def tokenize_dataframe(df):\n",
    "    return tokenizer(\n",
    "        df['text'].tolist(), \n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=512,  # Or choose a length that suits your dataset\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the train+validation and test sets\n",
    "train_val_encodings = tokenize_dataframe(train_val_df)\n",
    "test_encodings = tokenize_dataframe(test_df)\n",
    "\n",
    "# wrap the tokenized data in a dataset class that can be used by PyTorch for training:\n",
    "\n",
    "# Convert labels to list for compatibility\n",
    "train_val_labels = train_val_df['label'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "\n",
    "# Create the dataset for train+validation and test sets\n",
    "train_val_dataset = TextDataset(train_val_encodings, train_val_labels)\n",
    "test_dataset = TextDataset(test_encodings, test_labels)\n",
    "\n",
    "# Now, let's split the train_val_dataset into separate training and validation sets\n",
    "# Calculate the number of samples for the training set (90% of the train_val_dataset)\n",
    "num_train_samples = int(0.9 * len(train_val_dataset))\n",
    "\n",
    "# Calculate the number of samples for the validation set\n",
    "num_val_samples = len(train_val_dataset) - num_train_samples\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(train_val_dataset, [num_train_samples, num_val_samples], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initializing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    conf_mat = confusion_matrix(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': conf_mat.tolist()  # Convert to list for JSON serialization\n",
    "    }\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
    "        if 'confusion_matrix' in metrics:\n",
    "            print(\"Confusion Matrix:\")\n",
    "            print(metrics['confusion_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=2)  # Adjust num_labels as per your task\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=8,   \n",
    "    per_device_eval_batch_size=16,   \n",
    "    weight_decay=0.01,               \n",
    "    evaluation_strategy='epoch',     \n",
    "    save_strategy='epoch',           \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,     # Load the best model at the end of training\n",
    "    metric_for_best_model=\"accuracy\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with one model instance\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[MetricsCallback()]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WandB and Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project=\"TER\", entity=\"arisoy10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()  # Evaluate on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.evaluate()\n",
    "predictions = trainer.predict(val_dataset)\n",
    "##trainer.evaluate(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Step 2: Extract actual labels from the validation dataset 'val_dataset'\n",
    "actual_labels = [val_dataset[i]['labels'].item() for i in range(len(val_dataset))]\n",
    "\n",
    "# Step 3: Identify indices of false positives\n",
    "false_positives_indices = [i for i, (pred, actual) in enumerate(zip(predicted_labels, actual_labels)) if pred == 1 and actual == 0]\n",
    "\n",
    "# Step 4: Decode and print false positives for review\n",
    "for idx in false_positives_indices:\n",
    "    # Assuming your dataset returns PyTorch tensors, use `.numpy()` to convert them for decoding\n",
    "    input_ids = val_dataset[idx]['input_ids'].numpy()\n",
    "    decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "    print(f\"False Positive Text at index {idx}: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"Test Performance:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
